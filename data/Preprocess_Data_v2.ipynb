{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import docopt\n",
    "import trimesh\n",
    "import glob\n",
    "\n",
    "from trimesh.sample import sample_surface\n",
    "from toolkit.src.renderer import render_cvcam\n",
    "from toolkit.src.utility import show_img_arr\n",
    "\n",
    "!export DISPLAY=172.17.0.1:0.0\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimesh.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(input_path, output_dir='./out', scale=300, n=5000):\n",
    "    \"\"\"Passes command line arguments into utility function.\"\"\"\n",
    "#     arguments = docopt.docopt(__doc__)\n",
    "#     input_path = arguments['<input_path>']\n",
    "#     output_dir = arguments['--out']\n",
    "#     scale = float(arguments['--scale'])\n",
    "#     n = int(arguments['--n'])\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for path in glob.iglob(input_path):\n",
    "        if not path.endswith('.obj'):\n",
    "            print(\"Skipping: \", path)\n",
    "            continue\n",
    "\n",
    "        print(\"Processing file: \", path)\n",
    "            \n",
    "        mesh = trimesh.load(path)\n",
    "        mesh.apply_scale(scale/mesh.scale)\n",
    "        points = sample_surface(mesh, n)\n",
    "        points -= points.mean(axis=0)\n",
    "        output_path = os.path.join(output_dir, os.path.basename(path).split('.obj')[0] + '.npy')\n",
    "        np.save(output_path, points)\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path = './samples/*'\n",
    "# out_path = main(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age Gender Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD PATH OF INFO FILE FROM FACESCAPE\n",
    "ANNOT_FILE = '../../info_list_v1.txt'\n",
    "f = open(ANNOT_FILE)\n",
    "gender_map = {'f': 0, 'm': 1}\n",
    "\n",
    "AGE_GENDER_DICT = dict()\n",
    "for line in f:\n",
    "    id_label, gender, age = line.split('\\n')[0].split()\n",
    "    try:\n",
    "        AGE_GENDER_DICT[int(id_label)] = [ int(age), gender_map[gender] ]\n",
    "    except:\n",
    "        print(\"Skipping id: \", int(id_label), age, gender)\n",
    "        continue\n",
    "    \n",
    "print(len(AGE_GENDER_DICT.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD READ PATH FOR FACESCAPE\n",
    "READ_ROOT_PATH = '../data/'\n",
    "# ADD WRITE PATH FOR FACESCAPE\n",
    "WRITE_ROOT_PATH = '../data_1024_out/'\n",
    "\n",
    "ID_STARTS = [0, 101, 301, 501]\n",
    "ID_ENDS = [100, 300, 500, 847]\n",
    "\n",
    "# sampling hyperparameters\n",
    "scale=300\n",
    "# number of points to be sampled\n",
    "n=1024 \n",
    "\n",
    "for ID_START, ID_END in zip(ID_STARTS, ID_ENDS):\n",
    "    idx_range = range(ID_START, ID_END+1)\n",
    "\n",
    "    EXPR_NAME_DICT = dict()\n",
    "    ALL_ANNOTS = dict() # .npy file: identity, age, gender, expression {id, name}\n",
    "\n",
    "    print(\"START: \", ID_START, \" END: \", ID_END)\n",
    "\n",
    "    ANNOTS_FILE_SAVE_PATH = 'all_annots_{}_{}.npy'.format(ID_START, ID_END)\n",
    "    print(ANNOTS_FILE_SAVE_PATH)\n",
    "    \n",
    "    total_written = 0\n",
    "\n",
    "    for ix in idx_range:\n",
    "\n",
    "        if ix not in AGE_GENDER_DICT:\n",
    "            print(\"Skipping identity: \", ix)\n",
    "            print(\"+++++++++++++++\")\n",
    "\n",
    "            continue\n",
    "\n",
    "        READ_DIR_PATH = READ_ROOT_PATH + '{}/models_reg'.format(ix)\n",
    "        WRITE_DIR_PATH = WRITE_ROOT_PATH + '{}/models_reg'.format(ix)\n",
    "\n",
    "        os.makedirs(WRITE_DIR_PATH, exist_ok=True)\n",
    "\n",
    "        if not os.path.exists(WRITE_DIR_PATH):\n",
    "            # redundant check\n",
    "            print(\"Missing output dir: \", WRITE_DIR_PATH)\n",
    "\n",
    "        print(\"processing identity: \", ix, \" from: \", READ_DIR_PATH)\n",
    "\n",
    "        for name in os.listdir(READ_DIR_PATH):\n",
    "\n",
    "            file_name = os.path.join(READ_DIR_PATH, name)\n",
    "\n",
    "            if not file_name.endswith('.obj'):\n",
    "                #print(\"Skipping: \", file_name)\n",
    "                continue\n",
    "\n",
    "            #print(\"Processing file: \", file_name)\n",
    "            #print(\"------------------------------\")\n",
    "\n",
    "            mesh = trimesh.load(file_name)\n",
    "            mesh.apply_scale(scale/mesh.scale)\n",
    "            points = sample_surface(mesh, n)\n",
    "            points -= points.mean(axis=0)\n",
    "\n",
    "            output_name = os.path.basename(file_name).split('.obj')[0] + '.npy'\n",
    "\n",
    "            output_path = os.path.join(WRITE_DIR_PATH, output_name)\n",
    "\n",
    "            # Collecting annotations\n",
    "            expr_id, expr_name = name.split('_')[0], '-'.join(name.split('_')[1:]).lower()\n",
    "\n",
    "            if expr_id not in EXPR_NAME_DICT:\n",
    "                EXPR_NAME_DICT[expr_id] = expr_name\n",
    "            else:\n",
    "                if expr_name != EXPR_NAME_DICT[expr_id]:\n",
    "                    print(\"Debug! check mismatch.. \", expr_name, \" : \", EXPR_NAME_DICT[expr_id])\n",
    "\n",
    "            folder_id = ix\n",
    "            age, gender = AGE_GENDER_DICT[folder_id]\n",
    "\n",
    "\n",
    "            ALL_ANNOTS[output_path] = [folder_id, age, gender, expr_id, expr_name]\n",
    "\n",
    "            np.save(output_path, points)\n",
    "\n",
    "        num_written = len(os.listdir(WRITE_DIR_PATH))\n",
    "\n",
    "        total_written += num_written\n",
    "\n",
    "        print(\"Total files written: \", num_written, \" in \", WRITE_DIR_PATH, \" in all: \", total_written)\n",
    "        print(\"----------------------------------\")\n",
    "\n",
    "    # Writing all\n",
    "    print(\"Total files saved: \", len(ALL_ANNOTS.keys()), total_written, \" at: \", ANNOTS_FILE_SAVE_PATH)\n",
    "    np.save(ANNOTS_FILE_SAVE_PATH, ALL_ANNOTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "ROOT = \"./\"\n",
    "all_annotations = {}\n",
    "\n",
    "for file in os.listdir(ROOT):\n",
    "    if file.split(\".\")[-1] == \"npy\" and file.startswith(\"all_annots\"):\n",
    "        print(f\"Processing {file}\")\n",
    "        x = np.load(os.path.join(ROOT, file), allow_pickle=True).item()\n",
    "        \n",
    "        for key in x.keys():\n",
    "            all_annotations[key[3:]] = x[key]\n",
    "\n",
    "# MODIFY SAVE PATH AS REQUIRED\n",
    "np.save(\"./all_annotations1024.npy\", all_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = np.load(\"./all_annotations1024.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dict.item().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(test_dict.item().keys(), key=lambda x: int(x.split(\"/\")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(test_dict.item().keys(), key=lambda x: int(x.split(\"/\")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# ROOT = \"../\"\n",
    "# x = list(test_dict.item().keys())[0]\n",
    "# y = np.load(os.path.join(ROOT, x), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.item()['points'].shape # bs, n, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
